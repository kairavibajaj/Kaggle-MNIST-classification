# -*- coding: utf-8 -*-
"""Copy of CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SCs5NM70EqSm0BYGrxJ1gwkMGfGCMjwx
"""

!pip install PyDrive

import os
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1NdP0ofpJvW3Eq9V2lKkgAmCM2yXVClky'})



import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import to_categorical
from keras.preprocessing import image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from tqdm import tqdm

import keras
from keras.models import Model
from keras.layers import *
from keras import optimizers

train = pd.read_csv('/content/drive/MyDrive/Kaggle competition 1/train.csv')

test = pd.read_csv('/content/drive/MyDrive/Kaggle competition 1/test.csv')
test.drop(['Unnamed: 1568'], axis =1, inplace= True)
test.head()

plt.imshow(np.array(test.iloc[0, :]).reshape(28,56))

train.drop(['Unnamed: 1568'], axis =1, inplace= True
           )

train.head()

df_label = pd.read_csv("/content/drive/MyDrive/Kaggle competition 1/train_result.csv")
df_label.drop(['Index'], axis = 1, inplace = True)

df_label.head()

from sklearn.model_selection import train_test_split
X_train, X_cv, y_train, y_cv = train_test_split(train, df_label, 
                                                test_size = 0.2,
                                                random_state = 42)

X_train = np.array(X_train).reshape(40000, 1568) #(33600, 784)
X_cv = np.array(X_cv).reshape(10000, 1568) #(8400, 784)

test = np.array(test).reshape(10000, 1568)

print((min(X_train[1]) , max(X_train[1])))

X_train = X_train.astype('float32'); X_cv= X_cv.astype('float32'); test = test.astype('float32')
X_train /= 0.9960937500000001; X_cv /= 0.9960937500000001; test /= 0.9960937500000001

# Convert labels to One Hot Encoded
num_digits = 19
y_train = keras.utils.to_categorical(y_train, num_digits)
y_cv = keras.utils.to_categorical(y_cv, num_digits)

print(y_train[89]) 
print(y_train[3])

n_input = 1568 # number of features
n_hidden_1 = 300
n_hidden_2 = 100
n_hidden_3 = 100
n_hidden_4 = 200
num_digits = 19

Inp = Input(shape=(1568,))
x = Dense(n_hidden_1, activation='relu', name = "Hidden_Layer_1")(Inp)
x = Dense(n_hidden_2, activation='relu', name = "Hidden_Layer_2")(x)
x = Dense(n_hidden_3, activation='relu', name = "Hidden_Layer_3")(x)
x = Dense(n_hidden_4, activation='relu', name = "Hidden_Layer_4")(x)
output = Dense(num_digits, activation='softmax', name = "Output_Layer")(x)

model = Model(Inp, output)
model.summary()

learning_rate = 0.01
training_epochs = 30
batch_size = 100
sgd = optimizers.SGD(lr=learning_rate)

model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])

history1 = model.fit(X_train, y_train,
                     batch_size = batch_size,
                     epochs = training_epochs,
                     verbose = 2,
                     validation_data=(X_cv, y_cv))

Inp = Input(shape=(1568,))
x = Dense(n_hidden_1, activation='relu', name = "Hidden_Layer_1")(Inp)
x = Dense(n_hidden_2, activation='relu', name = "Hidden_Layer_2")(x)
x = Dense(n_hidden_3, activation='relu', name = "Hidden_Layer_3")(x)
x = Dense(n_hidden_4, activation='relu', name = "Hidden_Layer_4")(x)
output = Dense(num_digits, activation='softmax', name = "Output_Layer")(x)

# We rely on ADAM as our optimizing methodology
adam = keras.optimizers.Adam(lr=learning_rate)
model2 = Model(Inp, output)

model2.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history2 = model2.fit(X_train, y_train,
                      batch_size = batch_size,
                      epochs = training_epochs,
                      verbose = 2,
                      validation_data=(X_cv, y_cv))

test_pred = pd.DataFrame(model2.predict(test, batch_size=200))
test_pred = pd.DataFrame(test_pred.idxmax(axis = 1))
test_pred.index.name = 'Index'
test_pred = test_pred.rename(columns = {0: 'Class'}).reset_index()
test_pred['Index'] = test_pred['Index'] 

test_pred.head()

test_pred.to_csv('mnist_submission.csv', index = False)



model13 = Sequential()
model13.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1)))
model13.add(Conv2D(64, (3, 3), activation='relu'))
model13.add(MaxPooling2D(pool_size=(2, 2)))
model13.add(Dropout(0.25))
model13.add(Flatten())
model13.add(Dense(128, activation='relu'))
model13.add(Dropout(0.5))
model13.add(Dense(10, activation='softmax'))